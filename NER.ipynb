{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def fetch_news_article(api_key):\n",
        "    url = f'https://newsapi.org/v2/top-headlines?sources=bbc-news&apiKey={api_key}'\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        articles = response.json().get('articles')\n",
        "        if articles:\n",
        "            return articles[0].get('content')\n",
        "    return None\n",
        "\n",
        "# Replace 'your_api_key' with your actual News API key\n",
        "api_key = '33c078d8a2b5412f9425e47643fa2dcf'\n",
        "article = fetch_news_article(api_key)\n",
        "print(\"Article:\\n\", article)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISWbfmXJYZPw",
        "outputId": "5187b161-135a-48fb-d406-2295f2e152f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article:\n",
            " The World Anti-Doping Agency (Wada) says it is \"unfairly caught\" in a row between the US and China, with their geopolitical tensions spilling onto the Olympic stage. \r\n",
            "China's top swimmers have been â€¦ [+1809 chars]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spacy\n"
      ],
      "metadata": {
        "id": "DlIlesJ2bs1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "def extract_entities_spacy(text):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "spacy_entities = extract_entities_spacy(article)\n",
        "print(\"Entities from spaCy:\\n\", spacy_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsE1NwbaY3rd",
        "outputId": "48b6728d-e127-4d3e-9c09-93ae8e0d2b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities from spaCy:\n",
            " [('The World Anti-Doping Agency', 'ORG'), ('Wada', 'GPE'), ('US', 'GPE'), ('China', 'GPE'), ('Olympic', 'LOC'), ('China', 'GPE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK"
      ],
      "metadata": {
        "id": "brYOHbJNbvRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "def extract_entities_nltk(text):\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('maxent_ne_chunker')\n",
        "    nltk.download('words')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    entities = []\n",
        "    for sentence in sentences:\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        tags = nltk.pos_tag(words)\n",
        "        tree = nltk.ne_chunk(tags, binary=False)\n",
        "        for subtree in tree:\n",
        "            if isinstance(subtree, nltk.Tree):\n",
        "                entity = \" \".join([word for word, tag in subtree.leaves()])\n",
        "                entity_type = subtree.label()\n",
        "                entities.append((entity, entity_type))\n",
        "    return entities\n",
        "\n",
        "nltk_entities = extract_entities_nltk(article)\n",
        "print(\"Entities from NLTK:\\n\", nltk_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Erm0o8abf8L",
        "outputId": "b4f62bcd-5927-457f-c971-995f2adc1440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities from NLTK:\n",
            " [('Wada', 'ORGANIZATION'), ('US', 'ORGANIZATION'), ('China', 'GPE'), ('China', 'GPE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "comparison"
      ],
      "metadata": {
        "id": "bHqIk4PBbxvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_entities(spacy_entities, nltk_entities):\n",
        "    spacy_set = set(spacy_entities)\n",
        "    nltk_set = set(nltk_entities)\n",
        "\n",
        "    common = spacy_set & nltk_set\n",
        "    spacy_unique = spacy_set - nltk_set\n",
        "    nltk_unique = nltk_set - spacy_set\n",
        "\n",
        "    print(\"Common Entities:\\n\", common)\n",
        "    print(\"\\nEntities unique to spaCy:\\n\", spacy_unique)\n",
        "    print(\"\\nEntities unique to NLTK:\\n\", nltk_unique)\n",
        "\n",
        "compare_entities(spacy_entities, nltk_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8tDOMaDbljS",
        "outputId": "2d7b778b-a3b7-4fc4-e852-3007ffbd4841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common Entities:\n",
            " {('China', 'GPE')}\n",
            "\n",
            "Entities unique to spaCy:\n",
            " {('Olympic', 'LOC'), ('US', 'GPE'), ('The World Anti-Doping Agency', 'ORG'), ('Wada', 'GPE')}\n",
            "\n",
            "Entities unique to NLTK:\n",
            " {('Wada', 'ORGANIZATION'), ('US', 'ORGANIZATION')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0E4172kcbp_I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}